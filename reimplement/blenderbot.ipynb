{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uj-user/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/uj-user/Yo/hybrid-ltm/data_eval/context-fms/MS_DG/test.jsonl\"\n",
    "data_path = \"/home/uj-user/Yo/hybrid-ltm/data_eval/context-fms/MS_DG/test_last_session.jsonl\"\n",
    "# model_name = \"facebook/blenderbot-3B\"\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "# model_name = \"gpt2\"\n",
    "MODEL_MAX_LEN = 64\n",
    "TARGET_LEN = 16\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>```\\nDialogue Session #3:\\nDoctor:Hello Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>```\\nDialogue Session #3:\\nNeighbors A:Hey the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>```\\nDialogue Session #4:\\nCo-workers A:Hey, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>```\\nDialogue Session #2:\\nMentee:Mentor, I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>```\\nDialogue Session #5:\\nClassmates A:Sorry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>```\\nDialogue Session #4:\\nHusband:You know wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>```\\nDialogue Session #4:\\nClassmates B:Hey, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>```\\nDialogue Session #3:\\nClassmates A:I've b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>```\\nDialogue Session #2:\\nHusband:I found a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>```\\nDialogue Session #2:\\nPatient:Thank you s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input\n",
       "0   ```\\nDialogue Session #3:\\nDoctor:Hello Patien...\n",
       "1   ```\\nDialogue Session #3:\\nNeighbors A:Hey the...\n",
       "2   ```\\nDialogue Session #4:\\nCo-workers A:Hey, B...\n",
       "3   ```\\nDialogue Session #2:\\nMentee:Mentor, I ha...\n",
       "4   ```\\nDialogue Session #5:\\nClassmates A:Sorry ...\n",
       "..                                                ...\n",
       "75  ```\\nDialogue Session #4:\\nHusband:You know wh...\n",
       "76  ```\\nDialogue Session #4:\\nClassmates B:Hey, C...\n",
       "77  ```\\nDialogue Session #3:\\nClassmates A:I've b...\n",
       "78  ```\\nDialogue Session #2:\\nHusband:I found a s...\n",
       "79  ```\\nDialogue Session #2:\\nPatient:Thank you s...\n",
       "\n",
       "[80 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_json(data_path, lines=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey there, have you ever considered pursuing journalism as a career? Journalism? No, not really. Why do you ask?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "dataset['prep_input'] = dataset['input'].map(lambda x: '```'+ '```'.join(x.split('```')[1:-1]) +'```')\n",
    "text_list = dataset['prep_input'][1].split('Dialogue Session #')[1:] # dialogue session remove\n",
    "result_list = [re.sub(r'^\\d+:\\s*', '', line) for line in text_list] # #1 숫자 제거\n",
    "result_list = ['\\n'.join(line.split(':', 1)[1].strip() for line in lines.split('\\n') if ':' in line) for lines in result_list]\n",
    "result_list = [line.replace('\\n', ' ').replace('```', '').strip() for line in result_list] # 불필요 개행문자, 공백제거\n",
    "result_list = ' '.join(result_list)\n",
    "result_list = result_list[:-3].strip()\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlenderbotTokenizer(name_or_path='facebook/blenderbot-400M-distill', vocab_size=8008, model_max_length=128, is_fast=False, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t8008: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation_side='left', use_fast=False)\n",
    "# tokenizer.add_special_tokens({'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'})\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 2/80 [00:00<00:11,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " , a few years ago I went to Europe. That's great. Did you have any issues with your health during that trip? No, everything was fine. Although, I did experience some jet lag when I arrived back home.\n",
      "output:\n",
      " \n",
      "input:\n",
      "  Hey there, have you ever considered pursuing journalism as a career? Journalism? No, not really. Why do you ask?\n",
      "output:\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5/80 [00:00<00:08,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "  that, A. It's never easy losing a loved one. No, it's not. But I learned a lot about appreciating the people in my life while I had them, and that's something I try to remember every day.\n",
      "output:\n",
      " \n",
      "input:\n",
      " , Mentee. I'm glad you were able to offer some words of encouragement to personY. It's important to remember that everyone goes through tough times, but it's how we handle those tough times that defines us.\n",
      "output:\n",
      " \n",
      "input:\n",
      "  what happened? It's a complicated story, but I can share it with you sometime. For now, let's just enjoy our time together. Remember the time I helped you with your math homework? That was a good memory.\n",
      "output:\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [00:00<00:06, 10.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "  off of. Yeah, you're right. Thanks for your help, as always. Anytime, happy to be of assistance. Speaking of problem-solving, did you ever get that leak in your kitchen fixed?\n",
      "output:\n",
      " \n",
      "input:\n",
      "  remember. By the way, did you hear about the homeless shelter that opened up downtown recently? No, I hadn't heard about that. That's great news though. It's always important to have resources for those in need.\n",
      "output:\n",
      " \n",
      "input:\n",
      "  town. The main character is torn between her love for the town and her desire to pursue her dreams in the city. That sounds really interesting. I'm looking forward to reading it. Have you had any other writing projects recently?\n",
      "output:\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 10/80 [00:01<00:06, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " . That's great! Have you guys talked since then? Yeah, we've been talking and I'm really excited to see where it goes. I'm happy for you. Just make sure you're honest with her and treat her right.\n",
      "output:\n",
      " \n",
      "input:\n",
      "  skills by reading a variety of books. Remember when we talked about that a few days after I won that challenge? Of course, I do. That's great to hear. Did you learn anything new from the books you've read recently?\n",
      "output:\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 10/80 [00:01<00:08,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "  sorry, I didn't realize it was bothering you. It's just that I want to spend time with you too. Maybe we could read together or something? That's a good idea. We could start a book club or something.\n",
      "output:\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ans_jsons = []\n",
    "for i, line in enumerate(tqdm(dataset['input'])):\n",
    "    \n",
    "    dataset['prep_input'] = dataset['input'].map(lambda x: '```'+ '```'.join(x.split('```')[1:-1]) +'```')\n",
    "    text_list = dataset['prep_input'][i].split('Dialogue Session #')[1:] # dialogue session remove\n",
    "    result_list = [re.sub(r'^\\d+:\\s*', '', line) for line in text_list] # #1 숫자 제거\n",
    "    result_list = ['\\n'.join(line.split(':', 1)[1].strip() for line in lines.split('\\n') if ':' in line) for lines in result_list]\n",
    "    result_list = [line.replace('\\n', ' ').replace('```', '').strip() for line in result_list] # 불필요 개행문자, 공백제거\n",
    "    result_list = ' '.join(result_list)\n",
    "    result_list = result_list[:-3].strip()\n",
    "    # print(result_list)\n",
    "\n",
    "    input = tokenizer(result_list, max_length=MODEL_MAX_LEN-TARGET_LEN, truncation=True, add_special_tokens = False, return_tensors='pt').to(device)#.input_ids\n",
    "    output_ids = model.generate(\n",
    "        **input,\n",
    "        # do_sample=True,\n",
    "        # repetition_penalty=1.1,\n",
    "        # no_repeat_ngram_size=2,\n",
    "        # # temperature=0.2,\n",
    "        # top_p=0.8,\n",
    "        # max_length=MODEL_MAX_LEN,\n",
    "        encoder_no_repeat_ngram_size= None\n",
    "    )\n",
    "    output_ids = output_ids[0][len(input.input_ids[0]):]\n",
    "    outputs = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    print('input:\\n', tokenizer.decode(input.input_ids[0]))\n",
    "    print('output:\\n', outputs)\n",
    "    if i%20==10:\n",
    "        break\n",
    "\n",
    "    ans_jsons.append(\n",
    "        {\n",
    "            \"question_id\": i,\n",
    "            \"text\": outputs,\n",
    "            \"model_id\": model_name,\n",
    "            \"metadata\": {},\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"predictions/{model_name}_{data_path.split('/')[-2]}.json\"\n",
    "save_path = os.path.dirname(filename)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "df_ans_json = pd.DataFrame(ans_jsons)\n",
    "df_ans_json.to_json(filename, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "# mname = \"facebook/blenderbot-400M-distill\"\n",
    "# model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "# tokenizer = BlenderbotTokenizer.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Did you learn anything new from the books you've read recently? I'm with you on that one.</s>\"]\n"
     ]
    }
   ],
   "source": [
    "UTTERANCE = \"Did you learn anything new from the books you've read recently?\"\n",
    "# That's unfortunate. Are they trying to lose weight or are they just trying to be healthier? They are trying to get healthier, but it's hard for them to stick to it.\"\n",
    "inputs = tokenizer([UTTERANCE],add_special_tokens= False, return_tensors=\"pt\").to(device)\n",
    "reply_ids = model.generate(**inputs, encoder_no_repeat_ngram_size= None)\n",
    "print(tokenizer.batch_decode(reply_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
