{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-2tUMnZ2qrpS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/uj-user/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Od7kQ0teqrpU"
      },
      "outputs": [],
      "source": [
        "def prepare_MSC(dataset_path:str) -> pd.DataFrame:\n",
        "    print(dataset_path)\n",
        "    df = pd.read_json(dataset_path, lines=True)\n",
        "\n",
        "    multi_session = []\n",
        "    for idx in range(len(df)): # number of data\n",
        "        for sess in range(len(df['sessions'][idx])): # number of session\n",
        "            multi_session_personas1 = df['sessions'][idx][sess]['personas'][0]['text']\n",
        "            multi_session_personas2 = df['sessions'][idx][sess]['personas'][1]['text']\n",
        "\n",
        "            multi_session_dialogue = []\n",
        "            multi_session_speaker = []\n",
        "\n",
        "            # number of turn\n",
        "            for turn in range(len(df['sessions'][idx][sess]['dialogue'])):\n",
        "                multi_session_dialogue.append(df['sessions'][idx][sess]['dialogue'][turn]['text'])\n",
        "                multi_session_speaker.append(df['sessions'][idx][sess]['dialogue'][turn]['speaker'])\n",
        "\n",
        "            multi_session.append(['MSC', idx, sess, multi_session_personas1, multi_session_personas2, multi_session_dialogue, multi_session_speaker])\n",
        "\n",
        "    data = pd.DataFrame(multi_session, columns=['dataset', 'dialoug_id', 'session_id', 'persona1', 'persona2', 'dialogue', 'speaker'])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uPXdwDe9qrpV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../data/downloads/MSC/train.json\n",
            "../data/downloads/MSC/validation.json\n",
            "../data/downloads/MSC/test.json\n"
          ]
        }
      ],
      "source": [
        "dir_path= '../data/downloads'\n",
        "MSC={}\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    MSC[split] = prepare_MSC(f'{dir_path}/MSC/{split}.json')\n",
        "    MSC[split].to_json(f'../data/downloads/MSC/prerocessed_{split}.jsonl', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9wahKe_KqrpV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 375.34ba/s]\n",
            "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
            "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
            "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 296.58ba/s]\n",
            "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
            "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
            "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 282.31ba/s]\n",
            "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
            "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
            "Downloading metadata: 100%|██████████| 897/897 [00:00<00:00, 7.01MB/s]\n"
          ]
        }
      ],
      "source": [
        "def upload_to_huggingface(dataset: pd.DataFrame, save_name:str):\n",
        "    raw_train = Dataset.from_pandas(dataset['train'])\n",
        "    raw_valid = Dataset.from_pandas(dataset['validation'])\n",
        "    raw_test = Dataset.from_pandas(dataset['test'])\n",
        "    concat_dataset = DatasetDict({'train': raw_train, 'validation': raw_valid, 'test': raw_test})\n",
        "    concat_dataset.push_to_hub(save_name)\n",
        "\n",
        "huggingface_user_name='nayohan'\n",
        "upload_to_huggingface(MSC, f'{huggingface_user_name}/multi-session-chat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def _task_dialouge_generation(preprocessed_dataset:pd.DataFrame) -> pd.DataFrame:\n",
        "    df = preprocessed_dataset\n",
        "\n",
        "    task_dsg = []\n",
        "    for idx in range(len(df)): # number of data\n",
        "        multi_turn_dialogue = []\n",
        "        n_turn = len(df['dialogue'][idx])\n",
        "\n",
        "        for turn in range(n_turn):\n",
        "            row = f\"{df['speaker'][idx][turn]}: {df['dialogue'][idx][turn]} \\n\"\n",
        "            multi_turn_dialogue.append(row)\n",
        "\n",
        "        rand_idx = random.randint(2, turn)\n",
        "        multi_turn_dialogue_part = multi_turn_dialogue[:rand_idx]\n",
        "\n",
        "        last_response = multi_turn_dialogue_part[-1]\n",
        "        last_spaker = multi_turn_dialogue_part[-1].split(':')[0]\n",
        "        multi_turn_dialogue_part[-1] = last_spaker + ': ###\\n'\n",
        "        context = ''.join(multi_turn_dialogue_part)\n",
        "\n",
        "        prompt = f\"\"\"You will be shown a {len(multi_turn_dialogue_part)} turn dialogues between {df['speaker'][idx][0]} and {df['speaker'][idx][1]}. Please read and understand given Dialogue Session, then complete the task under the guidance of Task Introduction.\\n\\n\"\"\"\n",
        "        main_context = \"```\\nDialogue Session:\\n\" + context + \"\\n```\\n\\n\"\n",
        "        Task_Introduction = \"\"\"```\\nTask Introduction:\\nAfter reading the entire Dialogue Session, please create an appropriate response.\\n```\\n Task Result:\"\"\"\n",
        "\n",
        "        input = prompt + main_context + Task_Introduction\n",
        "        output = last_response\n",
        "        task_dsg.append([input, output])\n",
        "    return pd.DataFrame(task_dsg, columns=['input', 'output'])\n",
        "\n",
        "\n",
        "download_path= '../data/downloads'\n",
        "task_save_path = '../data/tasks'\n",
        "task_list = [\"DG\"]\n",
        "\n",
        "task_generator = DialogueRelateTaskGenerator()\n",
        "for task in task_list:\n",
        "    os.makedirs(f'{task_save_path}/{task}', exist_ok=True)\n",
        "    dataset_dict={}\n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        dataset_dict[split] = _task_dialouge_generation(MSC[split])\n",
        "        dataset_dict[split].to_json(f'{task_save_path}/{task}/{split}.jsonl', orient='records', lines=True)\n",
        "    # upload_to_huggingface(dataset_dict, f'{huggingface_user_name}/{task}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1zPrBI-qrpV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def task_dialouge_span_generation(preprocessed_dataset:pd.DataFrame) -> pd.DataFrame:\n",
        "    df = preprocessed_dataset\n",
        "\n",
        "    task_dsg = []\n",
        "    for idx in range(len(df)): # number of data\n",
        "        multi_turn_dialogue = []\n",
        "        n_turn = len(df['dialogue'][idx])\n",
        "\n",
        "        for turn in range(n_turn):\n",
        "            row = f\"{df['speaker'][idx][turn]}: {df['dialogue'][idx][turn]} \\n\"\n",
        "            multi_turn_dialogue.append(row)\n",
        "\n",
        "        rand_idx = random.randint(0, turn)\n",
        "        dialogue_span = multi_turn_dialogue[rand_idx]\n",
        "        multi_turn_dialogue[rand_idx] = multi_turn_dialogue[rand_idx].split(':')[0] + ': ###\\n'\n",
        "        context = ''.join(multi_turn_dialogue)\n",
        "\n",
        "        prompt = f\"\"\"You will be shown a {n_turn}turn conversation between {df['speaker'][idx][0]} and {df['speaker'][idx][1]}.\n",
        "Please read and understand given Dialogue Session, then complete the task under the guidance of Task Introduction.\\n\\n\"\"\"\n",
        "\n",
        "        Task_Introduction = \"\"\"\n",
        "        ```\n",
        "        Task Introduction:\n",
        "        After reading the entire conversation, please create an appropriate conversation in the parts marked ###.\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        input = prompt + \"```\\n Dialogue Session:\\n\" + context + \"\\n```\\n\\n\" + Task_Introduction\n",
        "        output = dialogue_span\n",
        "        task_dsg.append([input, output])\n",
        "\n",
        "    return pd.DataFrame(task_dsg, columns=['input', 'output'])\n",
        "\n",
        "T_DSG = task_dialouge_span_generation(MSC['train'])\n",
        "T_DSG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../data/downloads/conversation_chronicles/train.json\n",
            "../data/downloads/conversation_chronicles/validation.json\n",
            "../data/downloads/conversation_chronicles/test.json\n"
          ]
        }
      ],
      "source": [
        "def prepare_ConversationChronicle(dataset_path:str) -> pd.DataFrame:\n",
        "    print(dataset_path)\n",
        "    df = pd.read_json(dataset_path, lines=True)\n",
        "    df = df.sample(frac=0.01, random_state=2023).reset_index()\n",
        "\n",
        "    multi_session = []\n",
        "    for idx in range(len(df)): # number of data\n",
        "        column_name = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\"]\n",
        "        for sess_num, c_name in enumerate(column_name): # number of session\n",
        "            data_id = df['dataID'][idx]\n",
        "            relationship = df['relationship'][idx]\n",
        "            time_interval = df['time_interval'][idx][sess_num]\n",
        "            summarization = df['summary'][idx][sess_num]\n",
        "            dialogue = df[f'{c_name}_session_dialogue'][sess_num]\n",
        "            speaker = df[f'{c_name}_session_dialogue'][sess_num]\n",
        "            multi_session.append(['CC', data_id, idx, sess_num, relationship, time_interval, summarization, dialogue, speaker])\n",
        "\n",
        "    data = pd.DataFrame(multi_session, columns=['dataset', 'data_id', 'dialogue_id', 'session_id', 'relationship', 'time_interval', 'summarization', 'dialogue', 'speaker'])\n",
        "    return data\n",
        "\n",
        "download_path= '../data/downloads'\n",
        "dataset_name=\"conversation_chronicles\"\n",
        "MSC={}\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    MSC[split] = prepare_ConversationChronicle(f'{download_path}/{dataset_name}/{split}.json')\n",
        "    MSC[split].to_json(f'{download_path}/{dataset_name}/prerocessed_{split}.jsonl', orient='records', lines=True)\n",
        "# upload_to_huggingface(MSC, f'{huggingface_user_name}/conversation-chronicles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = MSC['train']\n",
        "\n",
        "num_partitions =5\n",
        "rows_per_partitoins = len(df) // num_partitions\n",
        "\n",
        "sub_dataframes = [df.iloc[i:i+rows_per_partitoins] for i in range(0, len(df), rows_per_partitoins)]\n",
        "len(sub_dataframes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8648     episode-150540\n",
              "15597     episode-31403\n",
              "6810     episode-120874\n",
              "17882    episode-210994\n",
              "3789     episode-161650\n",
              "              ...      \n",
              "10953    episode-255156\n",
              "224       episode-18218\n",
              "1772     episode-189067\n",
              "19178    episode-226276\n",
              "2636     episode-199463\n",
              "Name: dataID, Length: 200, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_json(f'{download_path}/{dataset_name}/{split}.json', lines=True)\n",
        "df = df.sample(frac=0.01, random_state=2023)\n",
        "df['dataID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['dataset', 'data_id', 'dialogue_id', 'session_id', 'relationship', 'time_interval', 'summarization', 'dialogue', 'speaker'],\n",
              "        num_rows: 40000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['dataset', 'data_id', 'dialogue_id', 'session_id', 'relationship', 'time_interval', 'summarization', 'dialogue', 'speaker'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['dataset', 'data_id', 'dialogue_id', 'session_id', 'relationship', 'time_interval', 'summarization', 'dialogue', 'speaker'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = load_dataset('nayohan/conversation_chronicles')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "relationship\n",
              "Classmates             13040\n",
              "Neighbors               9885\n",
              "Co-workers              5685\n",
              "Mentee and Mentor       3230\n",
              "Husband and Wife        2865\n",
              "Parent and Child        1360\n",
              "Patient and Doctor      1225\n",
              "Employee and Boss       1085\n",
              "Student and Teacher     1060\n",
              "Athlete and Coach        565\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(data['train'])\n",
        "df['relationship'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "time_interval\n",
              "Start                      8000\n",
              "A few weeks after          6536\n",
              "A few months after         6433\n",
              "A few days after           6417\n",
              "A few hours after          6323\n",
              "A couple of years after    5897\n",
              "A couple of years           394\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['time_interval'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "'index=False' is only valid when 'orient' is 'split' or 'table'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/uj-user/Yo/hybrid-ltm/src/prepare_raw_eda.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22554a55227d/home/uj-user/Yo/hybrid-ltm/src/prepare_raw_eda.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m dataset_dict[split]\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_json(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdataset_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22554a55227d/home/uj-user/Yo/hybrid-ltm/src/prepare_raw_eda.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m dataset_dict[split]\u001b[39m=\u001b[39mdataset_dict[split]\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m2023\u001b[39m)\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22554a55227d/home/uj-user/Yo/hybrid-ltm/src/prepare_raw_eda.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset_dict[split]\u001b[39m.\u001b[39;49mto_json(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataset_path\u001b[39m}\u001b[39;49;00m\u001b[39m/sampled_\u001b[39;49m\u001b[39m{\u001b[39;49;00msplit\u001b[39m}\u001b[39;49;00m\u001b[39m.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m, orient\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrecords\u001b[39;49m\u001b[39m'\u001b[39;49m, lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/pandas/core/generic.py:2532\u001b[0m, in \u001b[0;36mNDFrame.to_json\u001b[0;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[1;32m   2529\u001b[0m config\u001b[39m.\u001b[39mis_nonnegative_int(indent)\n\u001b[1;32m   2530\u001b[0m indent \u001b[39m=\u001b[39m indent \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 2532\u001b[0m \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mto_json(\n\u001b[1;32m   2533\u001b[0m     path_or_buf\u001b[39m=\u001b[39;49mpath_or_buf,\n\u001b[1;32m   2534\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2535\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[1;32m   2536\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   2537\u001b[0m     double_precision\u001b[39m=\u001b[39;49mdouble_precision,\n\u001b[1;32m   2538\u001b[0m     force_ascii\u001b[39m=\u001b[39;49mforce_ascii,\n\u001b[1;32m   2539\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[1;32m   2540\u001b[0m     default_handler\u001b[39m=\u001b[39;49mdefault_handler,\n\u001b[1;32m   2541\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[1;32m   2542\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2543\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2544\u001b[0m     indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m   2545\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2546\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   2547\u001b[0m )\n",
            "File \u001b[0;32m~/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/pandas/io/json/_json.py:147\u001b[0m, in \u001b[0;36mto_json\u001b[0;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_json\u001b[39m(\n\u001b[1;32m    131\u001b[0m     path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m     obj: NDFrame,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     mode: Literal[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    145\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m index \u001b[39mand\u001b[39;00m orient \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 147\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex=False\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is only valid when \u001b[39m\u001b[39m'\u001b[39m\u001b[39morient\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m lines \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    152\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlines\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword only valid when \u001b[39m\u001b[39m'\u001b[39m\u001b[39morient\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is records\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: 'index=False' is only valid when 'orient' is 'split' or 'table'"
          ]
        }
      ],
      "source": [
        "dataset_name = \"conversation_chronicles\"\n",
        "dataset_path = f\"/home/uj-user/Yo/hybrid-ltm/data/downloads/{dataset_name}\"\n",
        "\n",
        "dataset_dict = {}\n",
        "for split in ['train', 'validation', 'test']: \n",
        "    dataset_dict[split]=pd.read_json(f'{dataset_path}/{split}.json', lines=True)\n",
        "    dataset_dict[split]=dataset_dict[split].sample(frac=0.05, random_state=2023).reset_index()\n",
        "    dataset_dict[split].to_json(f'{dataset_path}/sampled_{split}.jsonl', orient='records', lines=True, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [00:00<00:00, 11252.18it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 11542.73it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 11278.92it/s]\n"
          ]
        }
      ],
      "source": [
        "def _task_multi_session_dialouge_generation(preprocessed_dataset:pd.DataFrame) -> pd.DataFrame:\n",
        "    df = preprocessed_dataset\n",
        "    df = df[df['time_interval']==\"Start\"].reset_index()\n",
        "    \n",
        "    task_dsg = []\n",
        "    for idx in  tqdm(range(len(df)), 5): # number of data\n",
        "        \n",
        "        multi_session_dialogue = []\n",
        "        for i in range(idx, idx+4):\n",
        "            multi_turn_dialogue = []\n",
        "            n_turn = len(df['dialogue'][idx])\n",
        "            for turn in range(n_turn):\n",
        "                row = f\"{df['speaker'][idx][turn]}: {df['dialogue'][idx][turn]} \\n\"\n",
        "                multi_turn_dialogue.append(row)\n",
        "            multi_session_dialouge.append(multi_turn_dialogue)\n",
        "\n",
        "\n",
        "\n",
        "        rand_idx = random.randint(2, turn)\n",
        "        multi_turn_dialogue_part = multi_turn_dialogue[:rand_idx]\n",
        "        last_response = multi_turn_dialogue_part[-1]\n",
        "        last_spaker = multi_turn_dialogue_part[-1].split(':')[0]\n",
        "        multi_turn_dialogue_part[-1] = last_spaker + ': ###\\n'\n",
        "        context = ''.join(multi_turn_dialogue_part)\n",
        "\n",
        "        prompt = f\"\"\"You will be shown a 5 session dialogues between {df['speaker'][idx][0]} and {df['speaker'][idx][1]}. Please read and understand given Dialogue Sessions, then complete the task under the guidance of Task Introduction.\\n\\n\"\"\"\n",
        "        \n",
        "        main_context_list = []\n",
        "        for  i in range(5):\n",
        "            main_context_list.append(f\"```\\nDialogue Session #{i}:\\n\" + context + \"\\n```\\n\\n\")\n",
        "        main_context = ''.join(main_context_list)\n",
        "        task_introduction = \"\"\"```\\nTask Introduction:\\nAfter reading the entire Dialogue Session, please create an appropriate response.\\n```\\n Task Result:\"\"\"\n",
        "\n",
        "        input = prompt + main_context + task_introduction\n",
        "        output = last_response\n",
        "        task_dsg.append([input, output])\n",
        "    return pd.DataFrame(task_dsg, columns=['input', 'output'])\n",
        "        \n",
        "download_path= '../data/downloads'\n",
        "task_save_path = '../data/tasks'\n",
        "task_list = [\"MS_DG\"]\n",
        "\n",
        "MSC = load_dataset(f'{huggingface_user_name}/conversation_chronicles')\n",
        "for task in task_list:\n",
        "    os.makedirs(f'{task_save_path}/{task}', exist_ok=True)\n",
        "    dataset_dict={}\n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        dataset_dict[split] = _task_multi_session_dialouge_generation(pd.DataFrame(MSC[split]))\n",
        "        dataset_dict[split].to_json(f'{task_save_path}/{task}/{split}.jsonl', orient='records', lines=True)\n",
        "    # upload_to_huggingface(dataset_dict, f'{huggingface_user_name}/{task}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random \n",
        "\n",
        "random.randint(2, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>asdf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0\n",
              "0  asdf\n",
              "1  asdf\n",
              "2  asdf\n",
              "3  asdf\n",
              "4  asdf\n",
              "5  asdf\n",
              "6  asdf\n",
              "7  asdf\n",
              "8  asdf\n",
              "9  asdf"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame(['asdf', 'asdf','asdf', 'asdf','asdf', 'asdf','asdf', 'asdf','asdf', 'asdf',])\n",
        "df.split"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FiD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
