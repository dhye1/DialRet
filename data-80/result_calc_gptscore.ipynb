{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./eval_result_api_reason_v3_all_2e/llama-13b_MS_DG+MS_DG.csv', './eval_result_api_reason_v3_all_2e/llama-13b_MS_DG+MT_DG.csv', './eval_result_api_reason_v3_all_2e/llama-33b_MS_DG+MS_DG.csv', './eval_result_api_reason_v3_all_2e/llama-33b_MS_DG+MT_DG.csv', './eval_result_api_reason_v3_all_2e/llama-7b_MS_DG+MS_DG.csv', './eval_result_api_reason_v3_all_2e/llama-7b_MS_DG+MT_DG.csv']\n",
      "llama-13b_MS_DG\n",
      "llama-13b_MS_DG\n",
      "llama-33b_MS_DG\n",
      "llama-33b_MS_DG\n",
      "llama-7b_MS_DG\n",
      "llama-7b_MS_DG\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>cross_domain</th>\n",
       "      <th>Humanness</th>\n",
       "      <th>Engagingness</th>\n",
       "      <th>Memorability</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-13b_MS_DG</td>\n",
       "      <td>MS_DG</td>\n",
       "      <td>6.40</td>\n",
       "      <td>5.95</td>\n",
       "      <td>5.20</td>\n",
       "      <td>6.25</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama-33b_MS_DG</td>\n",
       "      <td>MS_DG</td>\n",
       "      <td>6.70</td>\n",
       "      <td>5.65</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.75</td>\n",
       "      <td>6.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-7b_MS_DG</td>\n",
       "      <td>MS_DG</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-13b_MS_DG</td>\n",
       "      <td>MT_DG</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.25</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.05</td>\n",
       "      <td>6.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-33b_MS_DG</td>\n",
       "      <td>MT_DG</td>\n",
       "      <td>7.25</td>\n",
       "      <td>6.65</td>\n",
       "      <td>7.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-7b_MS_DG</td>\n",
       "      <td>MT_DG</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.05</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model cross_domain  Humanness  Engagingness  Memorability  \\\n",
       "0  llama-13b_MS_DG        MS_DG       6.40          5.95          5.20   \n",
       "1  llama-33b_MS_DG        MS_DG       6.70          5.65          6.50   \n",
       "2   llama-7b_MS_DG        MS_DG       6.00          5.50          4.30   \n",
       "3  llama-13b_MS_DG        MT_DG       7.40          6.25          6.85   \n",
       "4  llama-33b_MS_DG        MT_DG       7.25          6.65          7.20   \n",
       "5   llama-7b_MS_DG        MT_DG       6.70          6.05          5.80   \n",
       "\n",
       "   Specificity   Avg  \n",
       "0         6.25  5.95  \n",
       "1         5.75  6.15  \n",
       "2         5.20  5.25  \n",
       "3         6.05  6.64  \n",
       "4         6.80  6.98  \n",
       "5         5.40  5.99  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = './eval_result_reason-v3-fixedprompt-match'\n",
    "# file_path = './eval_result_reason-v3-fixedprompt-match-2'\n",
    "# file_path = './eval_result_reason-v3-fixedprompt-match-3'\n",
    "\n",
    "# file_path = './eval_result_reason-v3-fixedprompt-json'\n",
    "# file_path = './eval_result_reason-v3-fixedprompt-json-'\n",
    "\n",
    "folder_name = './eval_result_api_reason_v3_all_2e'\n",
    "result_list = glob(folder_name + '/*.csv')\n",
    "result_list.sort()\n",
    "\n",
    "print(result_list)\n",
    "\n",
    "gptscore_result = []#{'Humanness':[], 'Engagingness':[], 'Memorability':[], 'Specificity':[]}\n",
    "for result in result_list:\n",
    "    value = result.replace('.csv', '').split('+')\n",
    "    # print(value)\n",
    "    model_name = '_'.join(value[:-1]).split('/')[-1]\n",
    "    print(model_name)\n",
    "    cross_domain = value[-1]\n",
    "    \n",
    "    df = pd.read_csv(result)\n",
    "    # print(df[df['eval_indicator']=='Engagingness'])\n",
    "    \n",
    "    Humanness = df[df['eval_indicator']=='Humanness']['rating'].mean()\n",
    "    Engagingness = df[df['eval_indicator']=='Engagingness']['rating'].mean()\n",
    "    Memorability = df[df['eval_indicator']=='Memorability']['rating'].mean()\n",
    "    Specificity = df[df['eval_indicator']=='Specificity']['rating'].mean()\n",
    "    Avg = df['rating'].mean()\n",
    "    \n",
    "    gptscore_result.append({'model':model_name, 'cross_domain':cross_domain, \n",
    "                            'Humanness':round(Humanness,2),'Engagingness':round(Engagingness,2),\n",
    "                            'Memorability':round(Memorability,2),'Specificity':round(Specificity,2),\n",
    "                            'Avg':round(Avg,2)})\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame(gptscore_result)\n",
    "score_df = score_df.sort_values(by=['cross_domain', 'model'], ignore_index=True)\n",
    "score_df.to_csv(f'{folder_name}_gpts.csv')\n",
    "\n",
    "score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tmodel\tcross_domain\tHumanness\tEngagingness\tMemorability\tSpecificity\tAvg\n",
    "# 0\tllama-13b_MS_DG\tMS_DG\t7.29\t5.29\t6.94\t3.17\t5.64\n",
    "# 1\tllama-7b_MS_DG\tMS_DG\t6.40\t6.00\t5.50\t4.62\t5.55\n",
    "# 2\tllama-13b_MS_DG\tMT_DG\t5.56\t4.06\t4.78\t5.44\t4.96\n",
    "# 3\tllama-7b_MS_DG\tMT_DG\t5.80\t6.11\t6.29\t3.29\t5.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'GPTScore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'GPTScore'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscore_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGPTScore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Yo/hybrid-ltm/ltm-venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'GPTScore'"
     ]
    }
   ],
   "source": [
    "score_df['GPTScore'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.45, 6.79, 6.9 ],\n",
       "       [6.55, 6.15, 6.55],\n",
       "       [6.55, 7.15, 7.35],\n",
       "       [6.45, 6.55, 6.  ],\n",
       "       [5.89, 6.37, 6.05],\n",
       "       [5.35, 5.35, 5.  ],\n",
       "       [6.37, 6.3 , 6.15],\n",
       "       [5.  , 5.26, 5.1 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = [[6.43, 6.0, 6.89, 5.33, 7.0, 6.08, 6.73, 5.58],\n",
    "#         [6.55, 6.33, 7.38, 5.09, 6.67, 6.08, 6.8, 6.06],\n",
    "#         [8.38, 5.86, 7.5, 6.44, 6.92, 6.07, 7.0, 6.14]\n",
    "# ]\n",
    "data = [[6.45, 6.55, 6.55, 6.45, 5.89, 5.35, 6.37, 5.0],\n",
    "        [6.79, 6.15, 7.15, 6.55, 6.37, 5.35, 6.3, 5.26],\n",
    "        [6.9, 6.55, 7.35, 6.0, 6.05, 5.0, 6.15, 5.1]\n",
    "]\n",
    "data = np.array(data).transpose()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.193563616824515\n",
      "그룹 간에는 통계적으로 유의미한 차이가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def anova_test(*groups):\n",
    "    f_statistic, p_value = scipy.stats.f_oneway(*groups)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "# 예시 데이터 (세 개의 그룹)\n",
    "group1 = [25, 30, 35, 40, 45]\n",
    "group2 = [20, 28, 32, 38, 42]\n",
    "group3 = [15, 25, 30, 35, 40]\n",
    "\n",
    "# ANOVA 검정\n",
    "# result_pvalue = anova_test(group1, group2, group3)\n",
    "result_pvalue = anova_test(data[0], data[1])\n",
    "\n",
    "print(\"P-value:\", result_pvalue)\n",
    "\n",
    "# p-value가 0.05 이하인 경우, 적어도 하나의 그룹 간에는 통계적으로 유의미한 차이가 있다고 간주할 수 있습니다.\n",
    "if result_pvalue < 0.05:\n",
    "    print(\"적어도 하나의 그룹 간에는 통계적으로 유의미한 차이가 있습니다.\")\n",
    "else:\n",
    "    print(\"그룹 간에는 통계적으로 유의미한 차이가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross</th>\n",
       "      <th>Model</th>\n",
       "      <th>ROUGE</th>\n",
       "      <th>BertScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MS_DG</td>\n",
       "      <td>llama-7b_MT_DG</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>0.6806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS_DG</td>\n",
       "      <td>llama-7b_MT2</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.6886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS_DG</td>\n",
       "      <td>llama-7b_MS_DG</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.6929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MS_DG</td>\n",
       "      <td>llama-7b_MS2</td>\n",
       "      <td>0.3143</td>\n",
       "      <td>0.6837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MT_DG</td>\n",
       "      <td>llama-7b_MT_DG</td>\n",
       "      <td>0.3205</td>\n",
       "      <td>0.6946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MT_DG</td>\n",
       "      <td>llama-7b_MT2</td>\n",
       "      <td>0.3577</td>\n",
       "      <td>0.7073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MT_DG</td>\n",
       "      <td>llama-7b_MS_DG</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.6887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MT_DG</td>\n",
       "      <td>llama-7b_MS2</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.6785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cross           Model   ROUGE  BertScore\n",
       "0  MS_DG  llama-7b_MT_DG  0.2988     0.6806\n",
       "1  MS_DG    llama-7b_MT2  0.3140     0.6886\n",
       "2  MS_DG  llama-7b_MS_DG  0.3133     0.6929\n",
       "3  MS_DG    llama-7b_MS2  0.3143     0.6837\n",
       "4  MT_DG  llama-7b_MT_DG  0.3205     0.6946\n",
       "5  MT_DG    llama-7b_MT2  0.3577     0.7073\n",
       "6  MT_DG  llama-7b_MS_DG  0.3105     0.6887\n",
       "7  MT_DG    llama-7b_MS2  0.3150     0.6785"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('/home/uj-user/Yo/hybrid-ltm/data/eval_result/llama-7b+BS.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROUGE</th>\n",
       "      <th>BertScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3205</td>\n",
       "      <td>0.6946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3577</td>\n",
       "      <td>0.7073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.6887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.6785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROUGE  BertScore\n",
       "0  0.3205     0.6946\n",
       "1  0.3577     0.7073\n",
       "2  0.3105     0.6887\n",
       "3  0.3150     0.6785"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/home/uj-user/Yo/hybrid-ltm/data/eval_result/llama-7b+MT_DG_BS_bak.csv')\n",
    "df2 = pd.read_csv('/home/uj-user/Yo/hybrid-ltm/data/eval_result/llama-7b+MT_DG_BS.csv')\n",
    "df1 = df1.drop(columns=['Model'])\n",
    "df2 = df2.drop(columns=['Model'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for column ROUGE:\n",
      "T-statistic: -0.6751512587223705\n",
      "P-value: 0.5186131635861952\n",
      "Fail to reject the null hypothesis for column ROUGE: There is no significant difference.\n",
      "\n",
      "\n",
      "Test for column BertScore:\n",
      "T-statistic: -0.7241738477785662\n",
      "P-value: 0.4895922212199193\n",
      "Fail to reject the null hypothesis for column BertScore: There is no significant difference.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# 예제 데이터프레임 생성\n",
    "# data1 = {'A': [6.43, 6.0, 6.89, 5.33, 7.0, 6.08, 6.73, 5.58]}\n",
    "# data1 = {'A': [6.54, 6.32, 7.35, 5.08, 6.68, 6.08, 6.8, 6.06]}\n",
    "\n",
    "# data2 = {'A': [6.55, 6.33, 7.38, 5.09, 6.67, 6.08, 6.8, 6.06]}\n",
    "\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# df2 = pd.DataFrame(data2)\n",
    "\n",
    "# 열마다 t-검정 수행\n",
    "for column in df1.columns:\n",
    "    t_statistic, p_value = ttest_ind(df1[column], df2[column])\n",
    "    print(f'Test for column {column}:')\n",
    "    print(f'T-statistic: {t_statistic}')\n",
    "    print(f'P-value: {p_value}')\n",
    "\n",
    "    # P-value가 유의수준(예: 0.05)보다 작으면 귀무가설 기각\n",
    "    if p_value < 0.05:\n",
    "        print(f'Reject the null hypothesis for column {column}: There is a significant difference.')\n",
    "    else:\n",
    "        print(f'Fail to reject the null hypothesis for column {column}: There is no significant difference.')\n",
    "\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
