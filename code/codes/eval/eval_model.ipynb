{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f237890cb644eee8814d7ba6a309884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/home/uj-user/Yo/hybrid-ltm/model/llama-7b_MT_DG', \n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                            #  device_map=\"auto\",\n",
    "                                            #  max_memory=[23, 23, 23, 23],\n",
    "                                             load_in_8bit=False)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='/home/uj-user/Yo/hybrid-ltm/model/llama-7b_MT_DG', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/home/uj-user/Yo/hybrid-ltm/model/llama-7b_MT_DG', use_fast=False)\n",
    "\n",
    "# from transformers import LlamaTokenizer\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('daryl149/llama-2-7b-hf', truncation_side='left')\n",
    "# tokenizer.add_special_tokens({'pad_token': '</s>'})\n",
    "# tokenizer.eos_token = tokenizer.pad_token \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/uj-user/.cache/huggingface/datasets/json/MT_DG-e1e4da65bf530ac9/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206b437eba2c41378802ecec9c45d520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('/home/uj-user/Yo/hybrid-ltm/data/tasks/MT_DG')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0205e8975614c699c52b1e1516cccf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=96):   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c8b07e9f294335872ba74ba0274369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=96):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b001738201b349d890ca2292a026c9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=96):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_func(example):\n",
    "    return tokenizer(example['input'], truncation=True, max_length=2048, return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "tokenized_dataset = data.map(tokenize_func, num_proc=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pre = \"<s>\\n\"\n",
    "qa_link = \"\\n\"\n",
    "TarLen=256\n",
    "MaxLen = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized_dataset['validation']['input_ids'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-workers A: Actually, I was thinking about the time when you made a big mistake and I didn't ask for help. I think I was just too nervous to ask for help.\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "for input in data['validation']['input']:\n",
    "    \n",
    "    input = q_pre + input +qa_link\n",
    "    input_ids = tokenizer([input], truncation=True, max_length=(MaxLen - TarLen), add_special_tokens=False).input_ids\n",
    "    target_len = min(len(input_ids[0])+TarLen, MaxLen)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "    torch.as_tensor(input_ids).to(device),\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    max_length=target_len,\n",
    ")\n",
    "    # print(input)\n",
    "    output_ids = output_ids[0][len(input_ids[0]):]\n",
    "    print(tokenizer.decode(output_ids, skip_special_tokens=True).strip())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7967, 29899,  1287,   414,   319, 29901, 12823, 29892,   306,   471,\n",
       "         7291,  1048,   278,   931,   746,   366,  1754,   263,  4802, 10171,\n",
       "          322,   306,  3282, 29915, 29873,  2244,   363,  1371, 29889,   306,\n",
       "         1348,   306,   471,   925,  2086, 23547,   681,   304,  2244,   363,\n",
       "         1371, 29889,    13,    13,     2], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-workers A: Actually, I was thinking about the time when you made a big mistake and I didn't ask for help. I think I was just too nervous to ask for help.\n",
      "\n",
      "</s>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_ids).strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
